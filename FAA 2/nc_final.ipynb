{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Keras with a date difference for the task loading\n",
    "Started on day Mon, 19 Dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 641553\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_pickle(\"./data/nc_tasks.pkl\")\n",
    "\n",
    "reviews = dataset[\"text\"].to_numpy()\n",
    "labels = dataset[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Total examples:\", reviews.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training set size: 30000\n",
      "Validation set size: 10000\n",
      "Testing set size: 10000\n",
      "Unlabeled negative pool: 11384\n",
      "Unlabeled positive pool: 580169\n"
     ]
    }
   ],
   "source": [
    "val_split = 5000\n",
    "test_split = 5000\n",
    "train_split = 15000\n",
    "\n",
    "# Separating the negative and positive samples for manual stratification\n",
    "x_positives, y_positives = reviews[labels == 1], labels[labels == 1]\n",
    "x_negatives, y_negatives = reviews[labels == 0], labels[labels == 0]\n",
    "\n",
    "# Creating training, validation and testing splits\n",
    "x_val, y_val = (\n",
    "    tf.concat((x_positives[:val_split], x_negatives[:val_split]), 0),\n",
    "    tf.concat((y_positives[:val_split], y_negatives[:val_split]), 0),\n",
    ")\n",
    "x_test, y_test = (\n",
    "    tf.concat(\n",
    "        (\n",
    "            x_positives[val_split : val_split + test_split],\n",
    "            x_negatives[val_split : val_split + test_split],\n",
    "        ),\n",
    "        0,\n",
    "    ),\n",
    "    tf.concat(\n",
    "        (\n",
    "            y_positives[val_split : val_split + test_split],\n",
    "            y_negatives[val_split : val_split + test_split],\n",
    "        ),\n",
    "        0,\n",
    "    ),\n",
    ")\n",
    "x_train, y_train = (\n",
    "    tf.concat(\n",
    "        (\n",
    "            x_positives[val_split + test_split : val_split + test_split + train_split],\n",
    "            x_negatives[val_split + test_split : val_split + test_split + train_split],\n",
    "        ),\n",
    "        0,\n",
    "    ),\n",
    "    tf.concat(\n",
    "        (\n",
    "            y_positives[val_split + test_split : val_split + test_split + train_split],\n",
    "            y_negatives[val_split + test_split : val_split + test_split + train_split],\n",
    "        ),\n",
    "        0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Remaining pool of samples are stored separately. These are only labeled as and when required\n",
    "x_pool_positives, y_pool_positives = (\n",
    "    x_positives[val_split + test_split + train_split :],\n",
    "    y_positives[val_split + test_split + train_split :],\n",
    ")\n",
    "x_pool_negatives, y_pool_negatives = (\n",
    "    x_negatives[val_split + test_split + train_split :],\n",
    "    y_negatives[val_split + test_split + train_split :],\n",
    ")\n",
    "\n",
    "# Creating TF Datasets for faster prefetching and parallelization\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "pool_negatives = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_pool_negatives, y_pool_negatives)\n",
    ")\n",
    "pool_positives = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_pool_positives, y_pool_positives)\n",
    ")\n",
    "\n",
    "print(f\"Initial training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Testing set size: {len(test_dataset)}\")\n",
    "print(f\"Unlabeled negative pool: {len(pool_negatives)}\")\n",
    "print(f\"Unlabeled positive pool: {len(pool_positives)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/engine/base_preprocessing_layer.py\", line 123, in adapt_step  *\n        self.update_state(data)\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 475, in update_state  **\n        self._lookup_layer.update_state(self._preprocess(data))\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 570, in _preprocess\n        raise ValueError(\n\n    ValueError: When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, None) with rank=2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m vectorizer \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mTextVectorization(\n\u001b[1;32m      2\u001b[0m     \u001b[39m30000\u001b[39m, standardize\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlower_and_strip_punctuation\u001b[39m\u001b[39m\"\u001b[39m, output_sequence_length\u001b[39m=\u001b[39m\u001b[39m150\u001b[39m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[39m# Adapting the dataset\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m vectorizer\u001b[39m.\u001b[39;49madapt(\n\u001b[1;32m      6\u001b[0m     train_dataset\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m x, y: x, num_parallel_calls\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mAUTOTUNE)\u001b[39m.\u001b[39;49mbatch(\u001b[39m256\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvectorize_text\u001b[39m(text, label):\n\u001b[1;32m     11\u001b[0m     text \u001b[39m=\u001b[39m vectorizer(text)\n",
      "File \u001b[0;32m~/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/layers/preprocessing/text_vectorization.py:472\u001b[0m, in \u001b[0;36mTextVectorization.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madapt\u001b[39m(\u001b[39mself\u001b[39m, data, batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, steps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m     \u001b[39m\"\"\"Computes a vocabulary of string terms from tokens in a dataset.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[39m    Calling `adapt()` on a `TextVectorization` layer is an alternative to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[39m          argument is not supported with array inputs.\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49madapt(data, batch_size\u001b[39m=\u001b[39;49mbatch_size, steps\u001b[39m=\u001b[39;49msteps)\n",
      "File \u001b[0;32m~/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/engine/base_preprocessing_layer.py:258\u001b[0m, in \u001b[0;36mPreprocessingLayer.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m    257\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m--> 258\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_adapt_function(iterator)\n\u001b[1;32m    259\u001b[0m         \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m    260\u001b[0m             context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/v3/_wbjkcws0b9cz7yr6r68jyy00000gn/T/__autograph_generated_file5c3wyfr4.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__adapt_step\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m      9\u001b[0m data \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mnext\u001b[39m), (ag__\u001b[39m.\u001b[39mld(iterator),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     10\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_adapt_maybe_build, (ag__\u001b[39m.\u001b[39mld(data),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 11\u001b[0m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mupdate_state, (ag__\u001b[39m.\u001b[39;49mld(data),), \u001b[39mNone\u001b[39;49;00m, fscope)\n",
      "File \u001b[0;32m~/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/layers/preprocessing/text_vectorization.py:475\u001b[0m, in \u001b[0;36mTextVectorization.update_state\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_state\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[0;32m--> 475\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lookup_layer\u001b[39m.\u001b[39mupdate_state(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess(data))\n",
      "File \u001b[0;32m~/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/layers/preprocessing/text_vectorization.py:570\u001b[0m, in \u001b[0;36mTextVectorization._preprocess\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mif\u001b[39;00m inputs\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    569\u001b[0m     \u001b[39mif\u001b[39;00m inputs\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 570\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    571\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mWhen using `TextVectorization` to tokenize strings, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mthe input rank must be 1 or the last shape dimension \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    573\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmust be 1. Received: inputs.shape=\u001b[39m\u001b[39m{\u001b[39;00minputs\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    574\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwith rank=\u001b[39m\u001b[39m{\u001b[39;00minputs\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         )\n\u001b[1;32m    576\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    577\u001b[0m         inputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msqueeze(inputs, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/engine/base_preprocessing_layer.py\", line 123, in adapt_step  *\n        self.update_state(data)\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 475, in update_state  **\n        self._lookup_layer.update_state(self._preprocess(data))\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 570, in _preprocess\n        raise ValueError(\n\n    ValueError: When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, None) with rank=2\n"
     ]
    }
   ],
   "source": [
    "vectorizer = layers.TextVectorization(\n",
    "    30000, standardize=\"lower_and_strip_punctuation\", output_sequence_length=150\n",
    ")\n",
    "# Adapting the dataset\n",
    "vectorizer.adapt(\n",
    "    train_dataset.map(lambda x, y: x, num_parallel_calls=tf.data.AUTOTUNE).batch(256)\n",
    ")\n",
    "\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "    text = vectorizer(text)\n",
    "    return text, label\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    vectorize_text, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "pool_negatives = pool_negatives.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "pool_positives = pool_positives.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.batch(256).map(\n",
    "    vectorize_text, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "test_dataset = test_dataset.batch(256).map(\n",
    "    vectorize_text, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for merging new history objects with older ones\n",
    "def append_history(losses, val_losses, accuracy, val_accuracy, history):\n",
    "    losses = losses + history.history[\"loss\"]\n",
    "    val_losses = val_losses + history.history[\"val_loss\"]\n",
    "    accuracy = accuracy + history.history[\"binary_accuracy\"]\n",
    "    val_accuracy = val_accuracy + history.history[\"val_binary_accuracy\"]\n",
    "    return losses, val_losses, accuracy, val_accuracy\n",
    "\n",
    "\n",
    "# Plotter function\n",
    "def plot_history(losses, val_losses, accuracies, val_accuracies, optimizer):\n",
    "    plt.plot(losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.legend([\"train_loss\", \"val_loss\"])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(f\"model/active/{optimizer}_loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(accuracies)\n",
    "    plt.plot(val_accuracies)\n",
    "    plt.legend([\"train_accuracy\", \"val_accuracy\"])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.savefig(f\"model/active/{optimizer}_accuracy\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.models.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(150,)),\n",
    "            layers.Embedding(input_dim=30000, output_dim=128),\n",
    "            layers.Bidirectional(layers.LSTM(32, return_sequences=True)),\n",
    "            layers.GlobalMaxPool1D(),\n",
    "            layers.Dense(20, activation=\"relu\"),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_active_learning_models(\n",
    "    train_dataset,\n",
    "    pool_negatives,\n",
    "    pool_positives,\n",
    "    val_dataset,\n",
    "    test_dataset,\n",
    "    optimizer,\n",
    "    num_iterations=5,\n",
    "    sampling_size=10000,\n",
    "):\n",
    "\n",
    "    # Creating lists for storing metrics\n",
    "    losses, val_losses, accuracies, val_accuracies = [], [], [], []\n",
    "\n",
    "    model = create_model()\n",
    "    # We will monitor the false positives and false negatives predicted by our model\n",
    "    # These will decide the subsequent sampling ratio for every Active Learning loop\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\n",
    "            keras.metrics.BinaryAccuracy(),\n",
    "            keras.metrics.FalseNegatives(),\n",
    "            keras.metrics.FalsePositives(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Defining checkpoints.\n",
    "    # The checkpoint callback is reused throughout the training since it only saves the best overall model.\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        f\"model/active/{optimizer}_Model.h5\", save_best_only=True, verbose=1\n",
    "    )\n",
    "    # Here, patience is set to 4. This can be set higher if desired.\n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=4, verbose=1)\n",
    "\n",
    "    print(f\"Starting to train with {len(train_dataset)} samples\")\n",
    "    # Initial fit with a small subset of the training set\n",
    "    history = model.fit(\n",
    "        train_dataset.cache().shuffle(20000).batch(256),\n",
    "        epochs=40,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[checkpoint, early_stopping],\n",
    "    )\n",
    "\n",
    "    # Appending history\n",
    "    losses, val_losses, accuracies, val_accuracies = append_history(\n",
    "        losses, val_losses, accuracies, val_accuracies, history\n",
    "    )\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # Getting predictions from previously trained model\n",
    "        predictions = model.predict(test_dataset)\n",
    "\n",
    "        # Generating labels from the output probabilities\n",
    "        rounded = tf.where(tf.greater(predictions, 0.5), 1, 0)\n",
    "\n",
    "        # Evaluating the number of zeros and ones incorrrectly classified\n",
    "        _, _, false_negatives, false_positives = model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "        print(\"-\" * 100)\n",
    "        print(\n",
    "            f\"Number of zeros incorrectly classified: {false_negatives}, Number of ones incorrectly classified: {false_positives}\"\n",
    "        )\n",
    "\n",
    "        # This technique of Active Learning demonstrates ratio based sampling where\n",
    "        # Number of ones/zeros to sample = Number of ones/zeros incorrectly classified / Total incorrectly classified\n",
    "        if false_negatives != 0 and false_positives != 0:\n",
    "            total = false_negatives + false_positives\n",
    "            sample_ratio_ones, sample_ratio_zeros = (\n",
    "                false_positives / total,\n",
    "                false_negatives / total,\n",
    "            )\n",
    "        # In the case where all samples are correctly predicted, we can sample both classes equally\n",
    "        else:\n",
    "            sample_ratio_ones, sample_ratio_zeros = 0.5, 0.5\n",
    "\n",
    "        print(\n",
    "            f\"Sample ratio for positives: {sample_ratio_ones}, Sample ratio for negatives:{sample_ratio_zeros}\"\n",
    "        )\n",
    "\n",
    "        # Sample the required number of ones and zeros\n",
    "        sampled_dataset = pool_negatives.take(\n",
    "            int(sample_ratio_zeros * sampling_size)\n",
    "        ).concatenate(pool_positives.take(int(sample_ratio_ones * sampling_size)))\n",
    "\n",
    "        # Skip the sampled data points to avoid repetition of sample\n",
    "        pool_negatives = pool_negatives.skip(int(sample_ratio_zeros * sampling_size))\n",
    "        pool_positives = pool_positives.skip(int(sample_ratio_ones * sampling_size))\n",
    "\n",
    "        # Concatenating the train_dataset with the sampled_dataset\n",
    "        train_dataset = train_dataset.concatenate(sampled_dataset).prefetch(\n",
    "            tf.data.AUTOTUNE\n",
    "        )\n",
    "\n",
    "        print(f\"Starting training with {len(train_dataset)} samples\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        # We recompile the model to reset the optimizer states and retrain the model\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=optimizer,\n",
    "            metrics=[\n",
    "                keras.metrics.BinaryAccuracy(),\n",
    "                keras.metrics.FalseNegatives(),\n",
    "                keras.metrics.FalsePositives(),\n",
    "            ],\n",
    "        )\n",
    "        history = model.fit(\n",
    "            train_dataset.cache().shuffle(20000).batch(256),\n",
    "            validation_data=val_dataset,\n",
    "            epochs=40,\n",
    "            callbacks=[\n",
    "                checkpoint,\n",
    "                keras.callbacks.EarlyStopping(patience=4, verbose=1),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Appending the history\n",
    "        losses, val_losses, accuracies, val_accuracies = append_history(\n",
    "            losses, val_losses, accuracies, val_accuracies, history\n",
    "        )\n",
    "\n",
    "        # Loading the best model from this training loop\n",
    "        model = keras.models.load_model(f\"model/active/{optimizer}_Model.h5\")\n",
    "\n",
    "    # Plotting the overall history and evaluating the final model\n",
    "    plot_history(losses, val_losses, accuracies, val_accuracies, optimizer)\n",
    "    print(\"-\" * 100)\n",
    "    print(\n",
    "        \"Test set evaluation: \",\n",
    "        model.evaluate(test_dataset, verbose=0, return_dict=True),\n",
    "    )\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return model, model.evaluate(test_dataset, verbose=0, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "optimizer = SGD\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/engine/training.py\", line 1820, in test_function  *\n        return step_function(self, iterator)\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/engine/training.py\", line 1804, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/engine/training.py\", line 1792, in run_step  **\n        outputs = model.test_step(data)\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/engine/training.py\", line 1756, in test_step\n        y_pred = self(x, training=False)\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_2\" is incompatible with the layer: expected shape=(None, 150), found shape=(None, 300)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39moptimizer = \u001b[39m\u001b[39m{\u001b[39;00moptimizer\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m      7\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTest set evaluation: \u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m----> 8\u001b[0m     model\u001b[39m.\u001b[39;49mevaluate(test_dataset, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/v3/_wbjkcws0b9cz7yr6r68jyy00000gn/T/__autograph_generated_filett4nqv25.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/engine/training.py\", line 1820, in test_function  *\n        return step_function(self, iterator)\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/engine/training.py\", line 1804, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/engine/training.py\", line 1792, in run_step  **\n        outputs = model.test_step(data)\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/engine/training.py\", line 1756, in test_step\n        y_pred = self(x, training=False)\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/joaoantonio/Documents/TTR/news_cruncher_ml/env/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_2\" is incompatible with the layer: expected shape=(None, 150), found shape=(None, 300)\n"
     ]
    }
   ],
   "source": [
    "optimizer = \"SGD\"\n",
    "for optimizer in [\"SGD\", \"Adam\", \"RMSprop\", \"Adadelta\", \"Adagrad\", \"Adamax\", \"Nadam\"]:\n",
    "    model = keras.models.load_model(f\"model/lstm_32/{optimizer}_Model.h5\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"optimizer = {optimizer}\")\n",
    "    print(\n",
    "        \"Test set evaluation: \",\n",
    "        model.evaluate(test_dataset, verbose=0, return_dict=True),\n",
    "    )\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = \"Nadam\"\n",
    "model = keras.models.load_model(f\"model/lstm_32/{optimizer}_Model.h5\")\n",
    "# A string input\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "# Turn strings into vocab indices\n",
    "indices = vectorizer(inputs)\n",
    "# Turn vocab indices into predictions\n",
    "outputs = model(indices)\n",
    "\n",
    "# Our end to end model\n",
    "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
    "end_to_end_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "end_to_end_model.save('model/model_keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1f5d89d31c60f736a5f3bf2293aeb816f54929d0c7c26f09073a8ca847dae0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
