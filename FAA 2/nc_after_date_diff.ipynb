{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Keras with a date difference for the task loading\n",
    "Started on day Mon, 19 Dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 641553\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_pickle(\"./data/nc_tasks.pkl\")\n",
    "\n",
    "reviews = dataset[\"text\"].to_numpy()\n",
    "labels = dataset[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Total examples:\", reviews.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training set size: 30000\n",
      "Validation set size: 10000\n",
      "Testing set size: 10000\n",
      "Unlabeled negative pool: 11384\n",
      "Unlabeled positive pool: 580169\n"
     ]
    }
   ],
   "source": [
    "val_split = 5000\n",
    "test_split = 5000\n",
    "train_split = 15000\n",
    "\n",
    "# Separating the negative and positive samples for manual stratification\n",
    "x_positives, y_positives = reviews[labels == 1], labels[labels == 1]\n",
    "x_negatives, y_negatives = reviews[labels == 0], labels[labels == 0]\n",
    "\n",
    "# Creating training, validation and testing splits\n",
    "x_val, y_val = (\n",
    "    tf.concat((x_positives[:val_split], x_negatives[:val_split]), 0),\n",
    "    tf.concat((y_positives[:val_split], y_negatives[:val_split]), 0),\n",
    ")\n",
    "x_test, y_test = (\n",
    "    tf.concat(\n",
    "        (\n",
    "            x_positives[val_split : val_split + test_split],\n",
    "            x_negatives[val_split : val_split + test_split],\n",
    "        ),\n",
    "        0,\n",
    "    ),\n",
    "    tf.concat(\n",
    "        (\n",
    "            y_positives[val_split : val_split + test_split],\n",
    "            y_negatives[val_split : val_split + test_split],\n",
    "        ),\n",
    "        0,\n",
    "    ),\n",
    ")\n",
    "x_train, y_train = (\n",
    "    tf.concat(\n",
    "        (\n",
    "            x_positives[val_split + test_split : val_split + test_split + train_split],\n",
    "            x_negatives[val_split + test_split : val_split + test_split + train_split],\n",
    "        ),\n",
    "        0,\n",
    "    ),\n",
    "    tf.concat(\n",
    "        (\n",
    "            y_positives[val_split + test_split : val_split + test_split + train_split],\n",
    "            y_negatives[val_split + test_split : val_split + test_split + train_split],\n",
    "        ),\n",
    "        0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Remaining pool of samples are stored separately. These are only labeled as and when required\n",
    "x_pool_positives, y_pool_positives = (\n",
    "    x_positives[val_split + test_split + train_split :],\n",
    "    y_positives[val_split + test_split + train_split :],\n",
    ")\n",
    "x_pool_negatives, y_pool_negatives = (\n",
    "    x_negatives[val_split + test_split + train_split :],\n",
    "    y_negatives[val_split + test_split + train_split :],\n",
    ")\n",
    "\n",
    "# Creating TF Datasets for faster prefetching and parallelization\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "pool_negatives = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_pool_negatives, y_pool_negatives)\n",
    ")\n",
    "pool_positives = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_pool_positives, y_pool_positives)\n",
    ")\n",
    "\n",
    "print(f\"Initial training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Testing set size: {len(test_dataset)}\")\n",
    "print(f\"Unlabeled negative pool: {len(pool_negatives)}\")\n",
    "print(f\"Unlabeled positive pool: {len(pool_positives)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = layers.TextVectorization(\n",
    "    30000, standardize=\"lower_and_strip_punctuation\", output_sequence_length=300\n",
    ")\n",
    "# Adapting the dataset\n",
    "vectorizer.adapt(\n",
    "    train_dataset.map(lambda x, y: x, num_parallel_calls=tf.data.AUTOTUNE).batch(128)\n",
    ")\n",
    "\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "    text = vectorizer(text)\n",
    "    return text, label\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    vectorize_text, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "pool_negatives = pool_negatives.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "pool_positives = pool_positives.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.batch(128).map(\n",
    "    vectorize_text, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "test_dataset = test_dataset.batch(128).map(\n",
    "    vectorize_text, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for merging new history objects with older ones\n",
    "def append_history(losses, val_losses, accuracy, val_accuracy, history):\n",
    "    losses = losses + history.history[\"loss\"]\n",
    "    val_losses = val_losses + history.history[\"val_loss\"]\n",
    "    accuracy = accuracy + history.history[\"binary_accuracy\"]\n",
    "    val_accuracy = val_accuracy + history.history[\"val_binary_accuracy\"]\n",
    "    return losses, val_losses, accuracy, val_accuracy\n",
    "\n",
    "\n",
    "# Plotter function\n",
    "def plot_history(losses, val_losses, accuracies, val_accuracies, optimizer):\n",
    "    plt.plot(losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.legend([\"train_loss\", \"val_loss\"])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(f\"model/lstm_128/{optimizer}_loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(accuracies)\n",
    "    plt.plot(val_accuracies)\n",
    "    plt.legend([\"train_accuracy\", \"val_accuracy\"])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.savefig(f\"model/lstm_128/{optimizer}_accuracy\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.models.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(300,)),\n",
    "            layers.Embedding(input_dim=30000, output_dim=128),\n",
    "            layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n",
    "            layers.GlobalMaxPool1D(),\n",
    "            layers.Dense(20, activation=\"relu\"),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_active_learning_models(\n",
    "    train_dataset,\n",
    "    pool_negatives,\n",
    "    pool_positives,\n",
    "    val_dataset,\n",
    "    test_dataset,\n",
    "    optimizer,\n",
    "    num_iterations=5,\n",
    "    sampling_size=10000,\n",
    "):\n",
    "\n",
    "    # Creating lists for storing metrics\n",
    "    losses, val_losses, accuracies, val_accuracies = [], [], [], []\n",
    "\n",
    "    model = create_model()\n",
    "    # We will monitor the false positives and false negatives predicted by our model\n",
    "    # These will decide the subsequent sampling ratio for every Active Learning loop\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\n",
    "            keras.metrics.BinaryAccuracy(),\n",
    "            keras.metrics.FalseNegatives(),\n",
    "            keras.metrics.FalsePositives(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Defining checkpoints.\n",
    "    # The checkpoint callback is reused throughout the training since it only saves the best overall model.\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        f\"model/lstm_128/{optimizer}_Model.h5\", save_best_only=True, verbose=1\n",
    "    )\n",
    "    # Here, patience is set to 4. This can be set higher if desired.\n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=4, verbose=1)\n",
    "\n",
    "    print(f\"Starting to train with {len(train_dataset)} samples\")\n",
    "    # Initial fit with a small subset of the training set\n",
    "    history = model.fit(\n",
    "        train_dataset.cache().shuffle(20000).batch(128),\n",
    "        epochs=200,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[checkpoint, early_stopping],\n",
    "    )\n",
    "\n",
    "    # Appending history\n",
    "    losses, val_losses, accuracies, val_accuracies = append_history(\n",
    "        losses, val_losses, accuracies, val_accuracies, history\n",
    "    )\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # Getting predictions from previously trained model\n",
    "        predictions = model.predict(test_dataset)\n",
    "\n",
    "        # Generating labels from the output probabilities\n",
    "        rounded = tf.where(tf.greater(predictions, 0.5), 1, 0)\n",
    "\n",
    "        # Evaluating the number of zeros and ones incorrrectly classified\n",
    "        _, _, false_negatives, false_positives = model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "        print(\"-\" * 100)\n",
    "        print(\n",
    "            f\"Number of zeros incorrectly classified: {false_negatives}, Number of ones incorrectly classified: {false_positives}\"\n",
    "        )\n",
    "\n",
    "        # This technique of Active Learning demonstrates ratio based sampling where\n",
    "        # Number of ones/zeros to sample = Number of ones/zeros incorrectly classified / Total incorrectly classified\n",
    "        if false_negatives != 0 and false_positives != 0:\n",
    "            total = false_negatives + false_positives\n",
    "            sample_ratio_ones, sample_ratio_zeros = (\n",
    "                false_positives / total,\n",
    "                false_negatives / total,\n",
    "            )\n",
    "        # In the case where all samples are correctly predicted, we can sample both classes equally\n",
    "        else:\n",
    "            sample_ratio_ones, sample_ratio_zeros = 0.5, 0.5\n",
    "\n",
    "        print(\n",
    "            f\"Sample ratio for positives: {sample_ratio_ones}, Sample ratio for negatives:{sample_ratio_zeros}\"\n",
    "        )\n",
    "\n",
    "        # Sample the required number of ones and zeros\n",
    "        sampled_dataset = pool_negatives.take(\n",
    "            int(sample_ratio_zeros * sampling_size)\n",
    "        ).concatenate(pool_positives.take(int(sample_ratio_ones * sampling_size)))\n",
    "\n",
    "        # Skip the sampled data points to avoid repetition of sample\n",
    "        pool_negatives = pool_negatives.skip(int(sample_ratio_zeros * sampling_size))\n",
    "        pool_positives = pool_positives.skip(int(sample_ratio_ones * sampling_size))\n",
    "\n",
    "        # Concatenating the train_dataset with the sampled_dataset\n",
    "        train_dataset = train_dataset.concatenate(sampled_dataset).prefetch(\n",
    "            tf.data.AUTOTUNE\n",
    "        )\n",
    "\n",
    "        print(f\"Starting training with {len(train_dataset)} samples\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        # We recompile the model to reset the optimizer states and retrain the model\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=optimizer,\n",
    "            metrics=[\n",
    "                keras.metrics.BinaryAccuracy(),\n",
    "                keras.metrics.FalseNegatives(),\n",
    "                keras.metrics.FalsePositives(),\n",
    "            ],\n",
    "        )\n",
    "        history = model.fit(\n",
    "            train_dataset.cache().shuffle(20000).batch(128),\n",
    "            validation_data=val_dataset,\n",
    "            epochs=200,\n",
    "            callbacks=[\n",
    "                checkpoint,\n",
    "                keras.callbacks.EarlyStopping(patience=4, verbose=1),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Appending the history\n",
    "        losses, val_losses, accuracies, val_accuracies = append_history(\n",
    "            losses, val_losses, accuracies, val_accuracies, history\n",
    "        )\n",
    "\n",
    "        # Loading the best model from this training loop\n",
    "        model = keras.models.load_model(f\"model/lstm_128/{optimizer}_Model.h5\")\n",
    "\n",
    "    # Plotting the overall history and evaluating the final model\n",
    "    plot_history(losses, val_losses, accuracies, val_accuracies, optimizer)\n",
    "    print(\"-\" * 100)\n",
    "    print(\n",
    "        \"Test set evaluation: \",\n",
    "        model.evaluate(test_dataset, verbose=0, return_dict=True),\n",
    "    )\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return model, model.evaluate(test_dataset, verbose=0, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 300, 128)          3840000   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 300, 256)         263168    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                5140      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,108,329\n",
      "Trainable params: 4,108,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Starting to train with 30000 samples\n",
      "Epoch 1/200\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.6909 - binary_accuracy: 0.5343 - false_negatives_1: 4609.0000 - false_positives_1: 9362.0000\n",
      "Epoch 1: val_loss improved from inf to 0.69324, saving model to model/lstm_128/SGD_Model.h5\n",
      "235/235 [==============================] - 348s 1s/step - loss: 0.6909 - binary_accuracy: 0.5343 - false_negatives_1: 4609.0000 - false_positives_1: 9362.0000 - val_loss: 0.6932 - val_binary_accuracy: 0.5000 - val_false_negatives_1: 5000.0000 - val_false_positives_1: 0.0000e+00\n",
      "Epoch 2/200\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.6916 - binary_accuracy: 0.5254 - false_negatives_1: 7507.0000 - false_positives_1: 6730.0000\n",
      "Epoch 2: val_loss improved from 0.69324 to 0.69280, saving model to model/lstm_128/SGD_Model.h5\n",
      "235/235 [==============================] - 353s 1s/step - loss: 0.6916 - binary_accuracy: 0.5254 - false_negatives_1: 7507.0000 - false_positives_1: 6730.0000 - val_loss: 0.6928 - val_binary_accuracy: 0.5000 - val_false_negatives_1: 5000.0000 - val_false_positives_1: 0.0000e+00\n",
      "Epoch 3/200\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.6910 - binary_accuracy: 0.5290 - false_negatives_1: 7989.0000 - false_positives_1: 6140.0000\n",
      "Epoch 3: val_loss improved from 0.69280 to 0.69248, saving model to model/lstm_128/SGD_Model.h5\n",
      "235/235 [==============================] - 355s 2s/step - loss: 0.6910 - binary_accuracy: 0.5290 - false_negatives_1: 7989.0000 - false_positives_1: 6140.0000 - val_loss: 0.6925 - val_binary_accuracy: 0.5000 - val_false_negatives_1: 5000.0000 - val_false_positives_1: 0.0000e+00\n",
      "Epoch 4/200\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.6907 - binary_accuracy: 0.5317 - false_negatives_1: 8071.0000 - false_positives_1: 5978.0000\n",
      "Epoch 4: val_loss improved from 0.69248 to 0.69192, saving model to model/lstm_128/SGD_Model.h5\n",
      "235/235 [==============================] - 404s 2s/step - loss: 0.6907 - binary_accuracy: 0.5317 - false_negatives_1: 8071.0000 - false_positives_1: 5978.0000 - val_loss: 0.6919 - val_binary_accuracy: 0.5000 - val_false_negatives_1: 5000.0000 - val_false_positives_1: 0.0000e+00\n",
      "Epoch 5/200\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.6899 - binary_accuracy: 0.5425 - false_negatives_1: 8118.0000 - false_positives_1: 5607.0000\n",
      "Epoch 5: val_loss improved from 0.69192 to 0.69137, saving model to model/lstm_128/SGD_Model.h5\n",
      "235/235 [==============================] - 375s 2s/step - loss: 0.6899 - binary_accuracy: 0.5425 - false_negatives_1: 8118.0000 - false_positives_1: 5607.0000 - val_loss: 0.6914 - val_binary_accuracy: 0.5024 - val_false_negatives_1: 4967.0000 - val_false_positives_1: 9.0000\n",
      "Epoch 6/200\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.6893 - binary_accuracy: 0.5444 - false_negatives_1: 7558.0000 - false_positives_1: 6110.0000\n",
      "Epoch 6: val_loss improved from 0.69137 to 0.69093, saving model to model/lstm_128/SGD_Model.h5\n",
      "235/235 [==============================] - 360s 2s/step - loss: 0.6893 - binary_accuracy: 0.5444 - false_negatives_1: 7558.0000 - false_positives_1: 6110.0000 - val_loss: 0.6909 - val_binary_accuracy: 0.5076 - val_false_negatives_1: 4908.0000 - val_false_positives_1: 16.0000\n",
      "Epoch 7/200\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.6886 - binary_accuracy: 0.5486 - false_negatives_1: 7455.0000 - false_positives_1: 6088.0000\n",
      "Epoch 7: val_loss improved from 0.69093 to 0.69079, saving model to model/lstm_128/SGD_Model.h5\n",
      "235/235 [==============================] - 327s 1s/step - loss: 0.6886 - binary_accuracy: 0.5486 - false_negatives_1: 7455.0000 - false_positives_1: 6088.0000 - val_loss: 0.6908 - val_binary_accuracy: 0.5057 - val_false_negatives_1: 4929.0000 - val_false_positives_1: 14.0000\n",
      "Epoch 8/200\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.6877 - binary_accuracy: 0.5506 - false_negatives_1: 7574.0000 - false_positives_1: 5907.0000\n",
      "Epoch 8: val_loss improved from 0.69079 to 0.69030, saving model to model/lstm_128/SGD_Model.h5\n",
      "235/235 [==============================] - 328s 1s/step - loss: 0.6877 - binary_accuracy: 0.5506 - false_negatives_1: 7574.0000 - false_positives_1: 5907.0000 - val_loss: 0.6903 - val_binary_accuracy: 0.5152 - val_false_negatives_1: 4825.0000 - val_false_positives_1: 23.0000\n",
      "Epoch 9/200\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.6871 - binary_accuracy: 0.5545 - false_negatives_1: 7409.0000 - false_positives_1: 5956.0000\n",
      "Epoch 9: val_loss improved from 0.69030 to 0.68958, saving model to model/lstm_128/SGD_Model.h5\n",
      "235/235 [==============================] - 1976s 8s/step - loss: 0.6871 - binary_accuracy: 0.5545 - false_negatives_1: 7409.0000 - false_positives_1: 5956.0000 - val_loss: 0.6896 - val_binary_accuracy: 0.5224 - val_false_negatives_1: 4627.0000 - val_false_positives_1: 149.0000\n",
      "Epoch 10/200\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.6866 - binary_accuracy: 0.5567 - false_negatives_1: 7268.0000 - false_positives_1: 6031.0000\n",
      "Epoch 10: val_loss improved from 0.68958 to 0.68882, saving model to model/lstm_128/SGD_Model.h5\n",
      "235/235 [==============================] - 1120s 5s/step - loss: 0.6866 - binary_accuracy: 0.5567 - false_negatives_1: 7268.0000 - false_positives_1: 6031.0000 - val_loss: 0.6888 - val_binary_accuracy: 0.5382 - val_false_negatives_1: 4420.0000 - val_false_positives_1: 198.0000\n",
      "Epoch 11/200\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.6857 - binary_accuracy: 0.5624 - false_negatives_1: 7089.0000 - false_positives_1: 6038.0000\n",
      "Epoch 11: val_loss improved from 0.68882 to 0.68868, saving model to model/lstm_128/SGD_Model.h5\n",
      "235/235 [==============================] - 397s 2s/step - loss: 0.6857 - binary_accuracy: 0.5624 - false_negatives_1: 7089.0000 - false_positives_1: 6038.0000 - val_loss: 0.6887 - val_binary_accuracy: 0.5276 - val_false_negatives_1: 4559.0000 - val_false_positives_1: 165.0000\n",
      "Epoch 12/200\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.6849 - binary_accuracy: 0.5665 - false_negatives_1: 7249.0000 - false_positives_1: 5755.0000\n",
      "Epoch 12: val_loss improved from 0.68868 to 0.68779, saving model to model/lstm_128/SGD_Model.h5\n",
      "235/235 [==============================] - 368s 2s/step - loss: 0.6849 - binary_accuracy: 0.5665 - false_negatives_1: 7249.0000 - false_positives_1: 5755.0000 - val_loss: 0.6878 - val_binary_accuracy: 0.5385 - val_false_negatives_1: 4416.0000 - val_false_positives_1: 199.0000\n",
      "Epoch 13/200\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.6839 - binary_accuracy: 0.5675 - false_negatives_1: 7516.0000 - false_positives_1: 5460.0000\n",
      "Epoch 13: val_loss improved from 0.68779 to 0.68641, saving model to model/lstm_128/SGD_Model.h5\n",
      "235/235 [==============================] - 390s 2s/step - loss: 0.6839 - binary_accuracy: 0.5675 - false_negatives_1: 7516.0000 - false_positives_1: 5460.0000 - val_loss: 0.6864 - val_binary_accuracy: 0.5639 - val_false_negatives_1: 4108.0000 - val_false_positives_1: 253.0000\n",
      "Epoch 14/200\n",
      " 73/235 [========>.....................] - ETA: 4:06 - loss: 0.6824 - binary_accuracy: 0.5575 - false_negatives_1: 847.0000 - false_positives_1: 3288.0000"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "for optimizer in [\"SGD\", \"Adam\", \"RMSprop\", \"Adadelta\", \"Adagrad\", \"Adamax\", \"Nadam\"]:\n",
    "    model, results = train_active_learning_models(\n",
    "        train_dataset, pool_negatives, pool_positives, val_dataset, test_dataset, optimizer\n",
    "    )\n",
    "    results_dict[optimizer] = results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_FAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 15 2022, 18:25:35) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "540857b334b17b7c8f347a05d47e5b62911a011d1f5b5a830e95133110947623"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
